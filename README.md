# PrevisÃ£o de Sucesso de Startups - CompetiÃ§Ã£o Kaggle

**CompetiÃ§Ã£o:** [Inteli-M3] Campeonato 2025

---

## ğŸ† Melhor Modelo: 81.88% de AcurÃ¡cia no Kaggle

### VisÃ£o Geral

Este projeto alcanÃ§ou **81.88% de acurÃ¡cia** na competiÃ§Ã£o Kaggle utilizando uma abordagem de **Ensemble de VotaÃ§Ã£o MajoritÃ¡ria (Hard Voting)** com dois modelos Random Forest complementares. Esta abordagem simples e robusta superou mÃ©todos mais complexos como stacking e soft voting, demonstrando que simplicidade e diversidade de modelos sÃ£o mais eficazes que complexidade excessiva.

**Arquivo de SubmissÃ£o:** `submission_majority_vote.csv`

---

## ğŸ¯ Arquitetura do Modelo

### Ensemble de VotaÃ§Ã£o MajoritÃ¡ria (Hard Voting)

O modelo vencedor utiliza **votaÃ§Ã£o majoritÃ¡ria** entre dois modelos Random Forest treinados com diferentes configuraÃ§Ãµes:

#### Modelos Base:

1. **RF_Original** - Random Forest com features originais
   - **Features:** 31 features originais do dataset
   - **AcurÃ¡cia CV:** 79.88% Â± 3.85%
   - **HiperparÃ¢metros:**
     - `n_estimators`: 500
     - `max_depth`: 10
     - `min_samples_split`: 5
     - `min_samples_leaf`: 1
     - `max_features`: 'log2'
     - `class_weight`: None

2. **RF_Poly** - Random Forest com features polinomiais
   - **Features:** 46 features (31 originais + 15 polinomiais de grau 2)
   - **AcurÃ¡cia CV:** 79.42% Â± 3.96%
   - **HiperparÃ¢metros:**
     - `n_estimators`: 200
     - `max_depth`: None (sem limite)
     - `min_samples_split`: 10
     - `min_samples_leaf`: 2
     - `max_features`: 0.3
     - `class_weight`: 'balanced'

### Como Funciona a VotaÃ§Ã£o MajoritÃ¡ria

```python
# Cada modelo faz uma prediÃ§Ã£o binÃ¡ria (0 ou 1)
RF_Original prediz: [1, 0, 1, 0, ...]
RF_Poly prediz:     [1, 1, 1, 0, ...]

# VotaÃ§Ã£o majoritÃ¡ria: se ambos concordam, usa o voto
# Se discordam, pode usar desempate ou voto do modelo mais confiÃ¡vel
PrediÃ§Ã£o final:     [1, 0/1, 1, 0, ...]
```

**Vantagens da VotaÃ§Ã£o MajoritÃ¡ria:**
- âœ… DecisÃµes discretas (0 ou 1) evitam overfitting de probabilidades
- âœ… Robustez: erros de um modelo sÃ£o compensados pelo outro
- âœ… Simplicidade: sem meta-learner complexo
- âœ… Diversidade: diferentes features e hiperparÃ¢metros capturam padrÃµes complementares

---

## ğŸ“Š MÃ©tricas de Desempenho

### Resultados do Ensemble

| MÃ©trica | Valor | ObservaÃ§Ã£o |
|---------|-------|------------|
| **AcurÃ¡cia CV** | ~79.5% | MÃ©dia dos modelos base |
| **AcurÃ¡cia Kaggle** | **81.88%** | Resultado final na competiÃ§Ã£o |
| **Gap CV-Kaggle** | **+2.38pp** | Gap positivo indica excelente generalizaÃ§Ã£o |
| **PrediÃ§Ãµes** | 277 | 194 sucessos (70.0%), 83 falhas (30.0%) |

### ComparaÃ§Ã£o com Outras Abordagens

| Abordagem | AcurÃ¡cia Kaggle | Gap CV-Kaggle | Status |
|-----------|-----------------|---------------|--------|
| **Hard Voting (RF+RF)** | **81.88%** | **+2.38pp** | âœ… **MELHOR** |
| Soft Voting | 79.71% | +0.21pp | Bom |
| Weighted Ensemble | 79.71% | +0.21pp | Bom |
| Advanced RF | 78.99% | -0.11pp | OK |
| GridSearchCV RF | 78.26% | -2.24pp | Overfitting |
| Stacking (5 modelos) | 76.09% | -3.02pp | Overfitting severo |

**Insight CrÃ­tico:** MÃ©todos mais complexos (GridSearchCV, Stacking) tiveram **pior desempenho** devido a overfitting. A simplicidade da votaÃ§Ã£o majoritÃ¡ria foi a chave do sucesso.

---

## ğŸ’¡ Por Que Esta Abordagem Funciona

### 1. Diversidade de Modelos

**RF_Original vs RF_Poly:**
- **Features diferentes:** 31 vs 46 features
- **HiperparÃ¢metros diferentes:** Profundidade, nÃºmero de Ã¡rvores, regularizaÃ§Ã£o
- **PadrÃµes complementares:** Cada modelo captura aspectos diferentes dos dados

**Resultado:** Erros dos modelos sÃ£o **nÃ£o-correlacionados**, permitindo que um compense o outro.

### 2. VotaÃ§Ã£o MajoritÃ¡ria > Soft Voting

**Hard Voting (VotaÃ§Ã£o MajoritÃ¡ria):**
```python
# Cada modelo vota 0 ou 1
PrediÃ§Ã£o = maioria([modelo1.predict(), modelo2.predict()])
```

**Soft Voting (MÃ©dia de Probabilidades):**
```python
# MÃ©dia das probabilidades
PrediÃ§Ã£o = mÃ©dia([modelo1.predict_proba(), modelo2.predict_proba()]) > 0.5
```

**Por que Hard Voting Ã© melhor:**
- âœ… DecisÃµes discretas sÃ£o mais robustas
- âœ… Evita overfitting de probabilidades calibradas
- âœ… Gap positivo (+2.38pp) vs gap pequeno do soft voting (+0.21pp)

### 3. Robustez ao Overfitting

**EvidÃªncia:**
- **Gap positivo (+2.38pp):** Modelo generaliza MELHOR no teste que no treino
- **MÃ©todos complexos falharam:**
  - GridSearchCV: -2.24pp gap (overfitting)
  - Stacking: -3.02pp gap (overfitting severo)
- **Simplicidade vence:** Menos parÃ¢metros = menos overfitting

### 4. Teto de Performance

**11 submissÃµes testadas, NENHUMA superou 81.88%:**
- OtimizaÃ§Ã£o Bayesiana (LightGBM): 79.71%
- GridSearchCV (216 combinaÃ§Ãµes): 78.26%
- Stacking (5 modelos): 76.09%
- Threshold optimization: 78.99%

**ConclusÃ£o:** 81.88% representa o **teto de performance** para este dataset com as abordagens testadas.

---

## ğŸ”„ Como Reproduzir

### Passo 1: Preparar o Ambiente

```bash
# Instalar dependÃªncias
pip install numpy pandas scikit-learn matplotlib seaborn

# Verificar estrutura do projeto
ls data/
# Esperado: train.csv, test.csv, sample_submission.csv
```

### Passo 2: Executar o Script de Ensemble

```bash
# Gerar as submissÃµes de ensemble
python create_ensemble_submissions.py
```

**SaÃ­da esperada:**
- `submission_majority_vote.csv` - Hard voting (MELHOR - 81.88%)
- `submission_voting_ensemble.csv` - Soft voting (79.71%)
- `submission_weighted_ensemble.csv` - Weighted voting (79.71%)

### Passo 3: Submeter ao Kaggle

1. Fazer upload de `submission_majority_vote.csv` no Kaggle
2. Verificar formato: 277 linhas, colunas `id` e `labels`
3. Resultado esperado: **~81.88% de acurÃ¡cia**

### Estrutura do Arquivo de SubmissÃ£o

```csv
id,labels
0,1
1,0
2,1
...
276,1
```

---

## ğŸ“ Aprendizados Principais

### 1. Simplicidade > Complexidade

**O que funcionou:**
- âœ… Hard voting com 2 modelos Random Forest
- âœ… HiperparÃ¢metros simples e robustos
- âœ… Features originais (31) + features polinomiais (46)

**O que NÃƒO funcionou:**
- âŒ Stacking com 5 modelos e meta-learner
- âŒ GridSearchCV com 216 combinaÃ§Ãµes
- âŒ OtimizaÃ§Ã£o Bayesiana com 150 trials
- âŒ Feature engineering extensiva (>50 features)

**LiÃ§Ã£o:** Occam's Razor se aplica - a soluÃ§Ã£o mais simples Ã© frequentemente a melhor.

### 2. OtimizaÃ§Ã£o Excessiva Prejudica

**EvidÃªncia:**
- GridSearchCV (80.50% CV) â†’ 78.26% Kaggle (-2.24pp)
- Baseline (80.18% CV) â†’ 78.26% Kaggle (-1.92pp)
- **Resultado:** Mesma performance no Kaggle, mas GridSearchCV teve mais overfitting

**LiÃ§Ã£o:** Mais otimizaÃ§Ã£o â‰  melhor performance. Overfitting ao CV Ã© um risco real.

### 3. Gap CV-Kaggle Ã© Indicador CrÃ­tico

**Gaps Positivos (Boa GeneralizaÃ§Ã£o):**
- Hard voting: +2.38pp âœ…
- Soft voting: +0.21pp âœ…

**Gaps Negativos (Overfitting):**
- GridSearchCV: -2.24pp âŒ
- Stacking: -3.02pp âŒ

**LiÃ§Ã£o:** Gap positivo Ã© raro e valioso - indica que o modelo generaliza melhor que o esperado.

### 4. Diversidade de Modelos Ã© Essencial

**Por que 2 Random Forests funcionaram:**
- Features diferentes (31 vs 46)
- HiperparÃ¢metros diferentes (profundidade, regularizaÃ§Ã£o)
- Erros nÃ£o-correlacionados

**LiÃ§Ã£o:** Diversidade > Quantidade. 2 modelos diversos > 5 modelos similares.

### 5. Dataset Pequeno Tem Limites

**CaracterÃ­sticas:**
- 646 amostras de treino (pequeno)
- 31 features originais
- RuÃ­do inerente em prediÃ§Ã£o de sucesso de startups

**ImplicaÃ§Ãµes:**
- Teto de performance ~82%
- Modelos complexos overfitam facilmente
- Simplicidade Ã© crucial

---

## ğŸ“ Arquivos Relacionados

### Scripts Principais

| Arquivo | DescriÃ§Ã£o |
|---------|-----------|
| `create_ensemble_submissions.py` | Gera as 3 submissÃµes de ensemble |
| `run_rf_gridsearch_fast.py` | GridSearchCV RF (nÃ£o recomendado) |
| `run_stacking_ensemble.py` | Stacking ensemble (nÃ£o recomendado) |

### SubmissÃµes Geradas

| Arquivo | AcurÃ¡cia Kaggle | RecomendaÃ§Ã£o |
|---------|-----------------|--------------|
| `submission_majority_vote.csv` | **81.88%** | âœ… **USAR ESTE** |
| `submission_voting_ensemble.csv` | 79.71% | Alternativa |
| `submission_weighted_ensemble.csv` | 79.71% | Alternativa |

### DocumentaÃ§Ã£o

| Arquivo | ConteÃºdo |
|---------|----------|
| `FINAL_RESULTS_ANALYSIS.md` | AnÃ¡lise completa dos resultados |
| `JOURNEY_SUMMARY.md` | Jornada de 78.26% â†’ 81.88% |
| `SUBMISSION_COMPARISON.md` | ComparaÃ§Ã£o detalhada de todas as submissÃµes |

---

## ğŸ“‹ Project Overview

### Competition Objective

Predict startup success (binary classification) based on features including:
- Funding information (amounts, rounds, investors)
- Geographic location (state indicators)
- Industry category
- Milestone achievements
- Relationship networks

### Dataset Description

- **Training Set**: 647 startups with known outcomes
- **Test Set**: 278 startups requiring predictions
- **Features**: 32 columns (numeric and categorical)
- **Target**: Binary label (0 = failure, 1 = success)

### Target Metric

- **Primary**: Accuracy â‰¥ 80%
- **Secondary**: Precision, Recall, F1-score

---

## ğŸ›  Technical Stack

### Allowed Libraries

**Core ML/Data:**
- `numpy` - Numerical computations
- `pandas` - Data manipulation
- `scikit-learn` - Machine learning algorithms

**Visualization:**
- `matplotlib` - Primary visualization (required)
- `seaborn` - Optional styling enhancements

**Other:**
- `jupyter` - Interactive notebook environment

### Constraints

âœ“ No external data sources (only `data/` directory)  
âœ“ All preprocessing in pipelines (no data leakage)  
âœ“ Fixed random seeds (`random_state=42`)  
âœ“ Python 3.8+ compatible

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ data/                          # User-provided datasets
â”‚   â”œâ”€â”€ train.csv                  # Training data with labels
â”‚   â”œâ”€â”€ test.csv                   # Test data for predictions
â”‚   â””â”€â”€ sample_submission.csv      # Submission format template
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_startup_success.ipynb   # Main analysis notebook (12 sections)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py                # Package initialization
â”‚   â”œâ”€â”€ io_utils.py                # Data loading/saving utilities
â”‚   â”œâ”€â”€ features.py                # Feature engineering & preprocessing
â”‚   â”œâ”€â”€ modeling.py                # Model building & hyperparameter tuning
â”‚   â”œâ”€â”€ evaluation.py              # Metrics & cross-validation
â”‚   â””â”€â”€ cli.py                     # Command-line interface
â”œâ”€â”€ reports/                       # Generated reports (created by pipeline)
â”‚   â”œâ”€â”€ cv_metrics.csv             # Cross-validation results
â”‚   â””â”€â”€ best_rf_params.json        # Optimal RF hyperparameters
â”œâ”€â”€ Makefile                       # Automation commands
â”œâ”€â”€ README.md                      # This file
â””â”€â”€ submission.csv                 # Final predictions (generated)
```

---

## ğŸš€ Setup Instructions

### Local Environment

```bash
# Install dependencies
pip install numpy pandas scikit-learn matplotlib seaborn jupyter

# Verify data files exist
ls data/
# Expected: train.csv, test.csv, sample_submission.csv

# Verify project structure
python -c "from src.io_utils import load_data; print('âœ“ Setup complete!')"
```

### Kaggle Environment

1. Upload `notebooks/01_startup_success.ipynb` to Kaggle
2. Attach the competition dataset
3. Run all cells sequentially
4. Download `submission.csv`

---

## ğŸ’» Usage

### Option 1: Command-Line Interface (Recommended)

#### Using Makefile (Simplest)

```bash
# Run exploratory data analysis
make eda

# Cross-validation evaluation (all models)
make cv

# Hyperparameter tuning (Random Forest)
make tune

# Generate submission with default RF
make train

# Generate submission with tuned RF (recommended)
make train-best

# Quick submission (runs train-best)
make submit

# Run complete pipeline: eda â†’ cv â†’ tune â†’ submit
make all

# Clean generated files
make clean
```

#### Using Python CLI Directly

```bash
# Exploratory Data Analysis
python -m src.cli eda --data-dir data

# Cross-validation evaluation
python -m src.cli cv --data-dir data --output reports/cv_metrics.csv

# Hyperparameter tuning
python -m src.cli tune --data-dir data --seed 42 --output reports/best_rf_params.json

# Train and predict (default RF)
python -m src.cli train-predict --data-dir data --model rf --output submission.csv

# Train and predict (tuned RF)
python -m src.cli train-predict --data-dir data --use-best-rf --output submission.csv

# Train and predict (Gradient Boosting)
python -m src.cli train-predict --data-dir data --model gb --output submission.csv
```

### Option 2: Jupyter Notebook (Interactive)

```bash
# Launch Jupyter
jupyter notebook

# Open notebooks/01_startup_success.ipynb
# Run all cells sequentially (Cell â†’ Run All)
# Submission file will be generated in project root
```

---

## ğŸ“Š Pipeline Workflow

### 1. Exploratory Data Analysis (EDA)
- Dataset shapes and info
- Missing value analysis
- Feature type identification
- Target distribution
- Correlation analysis

### 2. Feature Engineering
- **Numeric features**: Median imputation + StandardScaler
- **Categorical features**: Mode imputation + OneHotEncoder (min_frequency=10)
- All transformations in `ColumnTransformer` (no data leakage)

### 3. Model Building
- **Logistic Regression**: Fast baseline
- **Random Forest**: Ensemble method (primary model)
- **Gradient Boosting**: Alternative ensemble

### 4. Cross-Validation
- 5-fold Stratified K-Fold
- Metrics: Accuracy, Precision, Recall, F1-score
- Results saved to `reports/cv_metrics.csv`

### 5. Hyperparameter Tuning
- RandomizedSearchCV (30 iterations, 5-fold CV)
- Search space: n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features
- Best parameters saved to `reports/best_rf_params.json`

### 6. Final Training & Prediction
- Train best model on 100% of training data
- Generate predictions for test set
- Create submission file matching required format

---

## âœ… Compliance Guarantees

| Requirement | Status | Details |
|------------|--------|---------|
| **Libraries** | âœ“ | Only numpy, pandas, scikit-learn for ML |
| **Visualization** | âœ“ | Only matplotlib (required) |
| **Data Sources** | âœ“ | Only `data/` directory |
| **Data Leakage** | âœ“ | All preprocessing in pipelines |
| **Reproducibility** | âœ“ | Fixed `random_state=42` |
| **Submission Format** | âœ“ | Matches `sample_submission.csv` exactly |

---

## ğŸ“ˆ Output Files

### Generated by Pipeline

| File | Description | Command |
|------|-------------|---------|
| `reports/cv_metrics.csv` | Cross-validation results for all models | `make cv` |
| `reports/best_rf_params.json` | Optimal Random Forest hyperparameters | `make tune` |
| `submission.csv` | Final predictions for Kaggle submission | `make submit` |

### Validation Checks

âœ“ `cv_metrics.csv` contains 3 rows (one per model)  
âœ“ `cv_metrics.csv` has columns: model, accuracy, precision, recall, f1  
âœ“ `submission.csv` has same columns as `sample_submission.csv`  
âœ“ `submission.csv` has same row count as `test.csv` (278 rows)  
âœ“ No missing values in submission  

---

## ğŸ¯ Expected Results

### Model Performance (Typical)

| Model | Accuracy | Precision | Recall | F1-Score |
|-------|----------|-----------|--------|----------|
| Logistic Regression | ~0.75-0.80 | ~0.70-0.78 | ~0.72-0.80 | ~0.71-0.79 |
| Random Forest | ~0.78-0.85 | ~0.75-0.83 | ~0.76-0.84 | ~0.75-0.83 |
| Gradient Boosting | ~0.76-0.82 | ~0.73-0.80 | ~0.74-0.81 | ~0.73-0.80 |

**Note:** Actual results depend on data characteristics and hyperparameter tuning.

### Threshold Achievement

- **Target**: â‰¥ 80% cross-validation accuracy
- **Expected**: Random Forest (tuned) typically meets or exceeds threshold

---

## ğŸ”§ Troubleshooting

### Common Issues

**Issue**: `ModuleNotFoundError: No module named 'src'`  
**Solution**: Run commands from project root directory

**Issue**: `FileNotFoundError: train.csv not found`  
**Solution**: Ensure data files are in `data/` directory

**Issue**: Notebook kernel crashes during tuning  
**Solution**: Reduce `n_iter` in `random_search_rf()` or use fewer CV folds

**Issue**: Makefile commands not working on Windows  
**Solution**: Use Python CLI directly or install `make` for Windows

---

## ğŸ“ Development Notes

### Code Quality Standards

- **Style**: PEP 8 compliant
- **Docstrings**: All functions documented
- **Type Hints**: Used where helpful
- **Error Handling**: Graceful failures with clear messages

### Testing Recommendations

```bash
# Test data loading
python -c "from src.io_utils import load_data; load_data('data')"

# Test preprocessing
python -c "from src.features import split_columns, build_preprocessor; import pandas as pd; df = pd.DataFrame({'a': [1,2], 'b': ['x','y']}); print(split_columns(df))"

# Test CLI
python -m src.cli --help
```

---

## ğŸ¤ Contributing

This project follows competition rules strictly. Suggested improvements:

1. **Feature Engineering**: Add interaction features, domain-specific ratios
2. **Model Exploration**: Try ensemble stacking, neural networks (if allowed)
3. **Hyperparameter Tuning**: Expand search space, use Bayesian optimization
4. **Validation**: Implement nested CV for unbiased estimates

---

## ğŸ“„ License

This project is created for educational purposes as part of the [Inteli-M3] Campeonato 2025 competition.

---

## ğŸ™ Acknowledgments

- **Competition Organizers**: [Inteli-M3] Campeonato 2025
- **Libraries**: scikit-learn, pandas, numpy, matplotlib
- **Community**: Kaggle community for inspiration and best practices

---

**Ready to compete? Run `make all` and submit your predictions!** ğŸš€

